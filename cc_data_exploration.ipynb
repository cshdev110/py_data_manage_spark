{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis of covid_cases.csv dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/system/.local/share/virtualenvs/code-1VgnQ5C9/lib/python3.9/site-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: \n",
      "Rows: 62407 and Columns: 59\n",
      "\n",
      "Column names:\n",
      "\n"
     ]
    },
    {
     "ename": "ParseException",
     "evalue": "\n[PARSE_SYNTAX_ERROR] Syntax error at or near end of input.(line 1, pos 2)\n\n== SQL ==\ndd\n--^^^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/system/unisq/CSC6002/assign3/code/cc_data_exploration.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/system/unisq/CSC6002/assign3/code/cc_data_exploration.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Printing column names\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/system/unisq/CSC6002/assign3/code/cc_data_exploration.ipynb#W2sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mColumn names:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/system/unisq/CSC6002/assign3/code/cc_data_exploration.ipynb#W2sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m spark\u001b[39m.\u001b[39;49mcreateDataFrame(dfs\u001b[39m.\u001b[39;49mcolumns, \u001b[39m'\u001b[39;49m\u001b[39mdd\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39mshow(\u001b[39mlen\u001b[39m(dfs\u001b[39m.\u001b[39mcolumns), truncate\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/system/unisq/CSC6002/assign3/code/cc_data_exploration.ipynb#W2sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# # Printing the schema\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/system/unisq/CSC6002/assign3/code/cc_data_exploration.ipynb#W2sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m dfs\u001b[39m.\u001b[39mprintSchema()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/code-1VgnQ5C9/lib/python3.9/site-packages/pyspark/sql/session.py:1391\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1385\u001b[0m     \u001b[39mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   1386\u001b[0m         error_class\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSHOULD_NOT_DATAFRAME\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1387\u001b[0m         message_parameters\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39marg_name\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m   1388\u001b[0m     )\n\u001b[1;32m   1390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(schema, \u001b[39mstr\u001b[39m):\n\u001b[0;32m-> 1391\u001b[0m     schema \u001b[39m=\u001b[39m cast(Union[AtomicType, StructType, \u001b[39mstr\u001b[39m], _parse_datatype_string(schema))\n\u001b[1;32m   1392\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(schema, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m   1393\u001b[0m     \u001b[39m# Must re-encode any unicode strings to be consistent with StructField names\u001b[39;00m\n\u001b[1;32m   1394\u001b[0m     schema \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mencode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(x, \u001b[39mstr\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m schema]\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/code-1VgnQ5C9/lib/python3.9/site-packages/pyspark/sql/types.py:1319\u001b[0m, in \u001b[0;36m_parse_datatype_string\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m   1317\u001b[0m     \u001b[39mreturn\u001b[39;00m from_ddl_datatype(\u001b[39m\"\u001b[39m\u001b[39mstruct<\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m>\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m s\u001b[39m.\u001b[39mstrip())\n\u001b[1;32m   1318\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:\n\u001b[0;32m-> 1319\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/code-1VgnQ5C9/lib/python3.9/site-packages/pyspark/sql/types.py:1309\u001b[0m, in \u001b[0;36m_parse_datatype_string\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m   1301\u001b[0m     \u001b[39mreturn\u001b[39;00m _parse_datatype_json_string(\n\u001b[1;32m   1302\u001b[0m         cast(JVMView, sc\u001b[39m.\u001b[39m_jvm)\n\u001b[1;32m   1303\u001b[0m         \u001b[39m.\u001b[39morg\u001b[39m.\u001b[39mapache\u001b[39m.\u001b[39mspark\u001b[39m.\u001b[39msql\u001b[39m.\u001b[39mapi\u001b[39m.\u001b[39mpython\u001b[39m.\u001b[39mPythonSQLUtils\u001b[39m.\u001b[39mparseDataType(type_str)\n\u001b[1;32m   1304\u001b[0m         \u001b[39m.\u001b[39mjson()\n\u001b[1;32m   1305\u001b[0m     )\n\u001b[1;32m   1307\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1308\u001b[0m     \u001b[39m# DDL format, \"fieldname datatype, fieldname datatype\".\u001b[39;00m\n\u001b[0;32m-> 1309\u001b[0m     \u001b[39mreturn\u001b[39;00m from_ddl_schema(s)\n\u001b[1;32m   1310\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1311\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1312\u001b[0m         \u001b[39m# For backwards compatibility, \"integer\", \"struct<fieldname: datatype>\" and etc.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/code-1VgnQ5C9/lib/python3.9/site-packages/pyspark/sql/types.py:1297\u001b[0m, in \u001b[0;36m_parse_datatype_string.<locals>.from_ddl_schema\u001b[0;34m(type_str)\u001b[0m\n\u001b[1;32m   1295\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_ddl_schema\u001b[39m(type_str: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataType:\n\u001b[1;32m   1296\u001b[0m     \u001b[39mreturn\u001b[39;00m _parse_datatype_json_string(\n\u001b[0;32m-> 1297\u001b[0m         cast(JVMView, sc\u001b[39m.\u001b[39;49m_jvm)\u001b[39m.\u001b[39;49morg\u001b[39m.\u001b[39;49mapache\u001b[39m.\u001b[39;49mspark\u001b[39m.\u001b[39;49msql\u001b[39m.\u001b[39;49mtypes\u001b[39m.\u001b[39;49mStructType\u001b[39m.\u001b[39;49mfromDDL(type_str)\u001b[39m.\u001b[39mjson()\n\u001b[1;32m   1298\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/code-1VgnQ5C9/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/code-1VgnQ5C9/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mParseException\u001b[0m: \n[PARSE_SYNTAX_ERROR] Syntax error at or near end of input.(line 1, pos 2)\n\n== SQL ==\ndd\n--^^^\n"
     ]
    }
   ],
   "source": [
    "# Creating a spark session\n",
    "spark = SparkSession.builder.appName('covid_cases').getOrCreate()\n",
    "\n",
    "# Reading the dataset with spark\n",
    "dfs = spark.read.csv('./datasets/covid_cases.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Printing the dataset size\n",
    "print(f'Dataset size: \\nRows: {dfs.count()} and Columns: {len(dfs.columns)}')\n",
    "\n",
    "# Printing column names\n",
    "print('\\nColumn names:\\n')\n",
    "spark.createDataFrame(dfs.columns, 'string').show(len(dfs.columns), truncate=False)\n",
    "\n",
    "# # Printing the schema\n",
    "dfs.printSchema()\n",
    "\n",
    "# # Printing the first 5 rows\n",
    "dfs.show(5)\n",
    "\n",
    "# # Printing the dataset size\n",
    "print(dfs.toPandas().shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code-1VgnQ5C9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
